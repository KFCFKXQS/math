{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1AQmlagiG5sxcanPH2BJm_V-qXaR0wKR7",
      "authorship_tag": "ABX9TyNnEFUI7y17e5blf3izEcMJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KFCFKXQS/math/blob/main/weak_supervision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "# 下载NLTK的相关资源\n",
        "nltk.download('punkt')  # 分词所需数据\n",
        "nltk.download('averaged_perceptron_tagger')  # 词性标注所需数据\n",
        "\n",
        "# 选择设备，如果有CUDA则使用CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "-JJmwoeDqmfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 1. glove_50d\n",
        "glove_50d={}\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/LSTM/glove.6B.50d.txt\") as f:\n",
        "    next(f)\n",
        "    for line in f:\n",
        "        line=line.split()\n",
        "        glove_50d[line[0]]=[float(xi) for xi in line[1:]]\n"
      ],
      "metadata": {
        "id": "Ye3lscANlXNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## 2.1 dataset_agnews\n",
        "# a、读取文档\n",
        "def clean_text(sentence):\n",
        "    cleaned_sentence = re.sub(r'[^\\w\\s]', '', sentence)  # 去除标点符号和特殊字符\n",
        "    cleaned_sentence = cleaned_sentence.lower()  # 转换为小写\n",
        "    tokens = nltk.word_tokenize(cleaned_sentence)  # 分词\n",
        "    tagged_tokens = nltk.pos_tag(tokens)  # 词性标注\n",
        "    # 保留动词和名词\n",
        "    cleaned_tokens = [token for token, pos in tagged_tokens if pos.startswith('NN')]\n",
        "    cleaned_sentence = ' '.join(cleaned_tokens)  # 拼接为字符串\n",
        "    return cleaned_sentence\n",
        "\n",
        "dataset_agnews_texts=[]\n",
        "dataset_agnews_reallabels=[]\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/dataset/agnews/dataset.csv') as f:\n",
        "    for line in f:\n",
        "        dataset_agnews_reallabels.append(int(line[0]))\n",
        "        dataset_agnews_texts.append(clean_text(line[1:]).strip().split())\n"
      ],
      "metadata": {
        "id": "KXyA4xTymOf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# b、读取已知类别和关键词\n",
        "dataset_agnews_classes={}\n",
        "dataset_agnews_keywords={}\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/dataset/agnews/classes.txt') as f:\n",
        "    for line in f:\n",
        "        line=line.strip().split(':')\n",
        "        dataset_agnews_classes[int(line[0])]=line[1]\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/dataset/agnews/keywords.txt') as f:\n",
        "    for line in f:\n",
        "        line = re.split(r\"[:,\\s]+\", line.strip())\n",
        "        dataset_agnews_keywords[int(line[0])]=set(line[1:])\n",
        "\n",
        "# c、构建词典和Word2Vec映射表.并将原文档中不在glove里的未知词改为<unk>\n",
        "dataset_agnews_words=set()\n",
        "dataset_agnews_word2vec={}\n",
        "dataset_agnews_vec2word={}\n",
        "dataset_agnews_processed_texts=[]\n",
        "\n",
        "unk_vector = [0] * 50\n",
        "for sentence in dataset_agnews_texts:\n",
        "    processed_sentence=[]\n",
        "    for word in sentence:\n",
        "        if word in glove_50d.keys():\n",
        "            dataset_agnews_words.add(word)\n",
        "            vec = glove_50d[word]\n",
        "            dataset_agnews_word2vec[word] = vec\n",
        "            dataset_agnews_vec2word[tuple(vec)] = word\n",
        "            processed_sentence.append(word)\n",
        "\n",
        "        else:\n",
        "            # 把不在glove50d的词 改成<unk>\n",
        "            dataset_agnews_words.add(\"<unk>\")\n",
        "            dataset_agnews_word2vec[\"<unk>\"] = unk_vector\n",
        "            dataset_agnews_vec2word[tuple(unk_vector)] = \"<unk>\"\n",
        "            processed_sentence.append(\"<unk>\")\n",
        "    dataset_agnews_processed_texts.append(processed_sentence)\n",
        "\n",
        "cosine_similarity_matrix=cosine_similarity([vec for vec in dataset_agnews_word2vec.values()])\n",
        "keys_list = list(dataset_agnews_word2vec.keys())"
      ],
      "metadata": {
        "id": "bgWDSFdDmTjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_agnews_texts[:3])"
      ],
      "metadata": {
        "id": "GiIzQykEm1Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 扩充关键词\n",
        "for label in dataset_agnews_keywords.keys():\n",
        "    expanded_keywords = set(dataset_agnews_keywords[label])\n",
        "    for word in dataset_agnews_keywords[label]:\n",
        "        word_index = keys_list.index(word)\n",
        "        similar_indices = np.where(cosine_similarity_matrix[word_index] > 0.8)[0]\n",
        "        for similar_index in similar_indices:\n",
        "            similar_word = keys_list[similar_index]\n",
        "            expanded_keywords.add(similar_word)\n",
        "    dataset_agnews_keywords[label] = expanded_keywords\n"
      ],
      "metadata": {
        "id": "RUK6azfvl1kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 计算每个句子的类别频率\n",
        "sentence_class_freq = []\n",
        "for sentence in dataset_agnews_processed_texts:\n",
        "    freq = [0]*len(dataset_agnews_classes)\n",
        "    for word in sentence:\n",
        "        for key, keywords in dataset_agnews_keywords.items():\n",
        "            if word in keywords:\n",
        "                freq[key] += 1\n",
        "    sentence_class_freq.append(freq)\n",
        "\n",
        "# 根据频率赋予句子类别标签\n",
        "sentence_class_pseudolabels = []\n",
        "for freq in sentence_class_freq:\n",
        "    max_index = np.argmax(freq)\n",
        "    max_value = freq[max_index]\n",
        "    sum_other_frequencies = sum(f for i, f in enumerate(freq) if i != max_index)\n",
        "    if max_value >2 * sum_other_frequencies:\n",
        "        sentence_class_pseudolabels.append(max_index)\n",
        "    else:\n",
        "        sentence_class_pseudolabels.append(-1)\n"
      ],
      "metadata": {
        "id": "XihFv4eompEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 计算准确率\n",
        "correct_classified=0\n",
        "total_classified=0\n",
        "\n",
        "for assigned_label, real_label in zip(sentence_class_pseudolabels, dataset_agnews_reallabels):\n",
        "    if assigned_label != -1:\n",
        "        total_classified += 1\n",
        "        if assigned_label == real_label:\n",
        "            correct_classified += 1\n",
        "\n",
        "print(\"Total classified: \", total_classified)\n",
        "print(\"Correct classified: \", correct_classified)\n",
        "print(\"Accuracy: \", correct_classified / total_classified)\n"
      ],
      "metadata": {
        "id": "RxWOaX2CnbOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 把每句话的词转化为数字编码\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return idxs\n",
        "\n",
        "inputs = [prepare_sequence(sent,dataset_agnews_word2vec) for sent in dataset_agnews_processed_texts]\n"
      ],
      "metadata": {
        "id": "79Djd44_ne9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 前面标记-1的表示未打标签，去掉它们\n",
        "origin_training_texts=[]\n",
        "origin_training_labels=[]\n",
        "for i in range(len(inputs)):\n",
        "  if sentence_class_pseudolabels[i]!=-1:\n",
        "    origin_training_texts.append(list(inputs[i]))\n",
        "    origin_training_labels.append(list(sentence_class_pseudolabels)[i])"
      ],
      "metadata": {
        "id": "l8YefI7LtGWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 50\n",
        "# 隐藏层的维度为64\n",
        "hidden_size = 64\n",
        "# 输出的维度4\n",
        "output_size = len(list(dataset_agnews_classes.keys()))\n"
      ],
      "metadata": {
        "id": "w7jq0LrAnYzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义模型\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.hidden2out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        packed_x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True)\n",
        "        packed_output, _ = self.lstm(packed_x)\n",
        "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "        output = self.hidden2out(output[:, -1, :])\n",
        "        output = self.softmax(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# 构建模型\n",
        "model = LSTMClassifier(input_size, hidden_size, output_size).to(device)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.5)\n"
      ],
      "metadata": {
        "id": "9Hivvn6qn3lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # 对输入序列进行填充\n",
        "    sorted_batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
        "    sequences = [torch.tensor(x[0]) for x in sorted_batch]\n",
        "    sequences_padded = pad_sequence(sequences, batch_first=True)\n",
        "    lengths = torch.LongTensor([len(x[0]) for x in sorted_batch])\n",
        "    labels = torch.LongTensor([x[1] for x in sorted_batch])\n",
        "    return sequences_padded, labels, lengths\n",
        "\n",
        "\n",
        "# 原始数据和标签\n",
        "origin_data = list(zip(origin_training_texts, origin_training_labels))\n",
        "\n",
        "# 划分训练集和测试集\n",
        "train_data, test_data = train_test_split(origin_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# 创建DataLoader\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
        "\n"
      ],
      "metadata": {
        "id": "DD665uMYukew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, test_loader, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_inputs, batch_labels, batch_lengths in train_loader:\n",
        "            batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)\n",
        "            # Keep lengths on CPU\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_inputs, batch_lengths)\n",
        "            loss = criterion(output, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "        print(f'Epoch {epoch + 1}/{epochs} Loss: {loss.item()} Test Loss: {test_loss} Test Acc: {test_acc}')\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_inputs, batch_labels, batch_lengths in test_loader:\n",
        "            batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)\n",
        "            # Keep lengths on CPU\n",
        "            output = model(batch_inputs, batch_lengths)\n",
        "            loss = criterion(output, batch_labels)\n",
        "            total_loss += loss.item() * batch_inputs.size(0)\n",
        "            predicted = torch.argmax(output, axis=1)\n",
        "            correct = (predicted == batch_labels).sum().item()\n",
        "            total_correct += correct\n",
        "            total_count += batch_inputs.size(0)\n",
        "\n",
        "    # 计算平均损失和精度\n",
        "    avg_loss = total_loss / total_count\n",
        "    accuracy = total_correct / total_count\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "\n",
        "# 开始训练\n",
        "train(model, train_loader, test_loader, criterion, optimizer, epochs=1000)\n"
      ],
      "metadata": {
        "id": "I5RyDaXCu-4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 自学习阶段\n",
        "high_confidence_threshold = 0.9\n",
        "\n",
        "while True:\n",
        "    high_confidence_samples = []\n",
        "    for i, text in enumerate(dataset_agnews_processed_texts):\n",
        "        if sentence_class_pseudolabels[i] == -1:  # 未标签的数据\n",
        "            with torch.no_grad():\n",
        "                text_tensor = torch.tensor([prepare_sequence(text, dataset_agnews_word2vec)]).float().to(device)\n",
        "                prediction = model(text_tensor, [len(text)])\n",
        "                probabilities, predicted = torch.max(torch.exp(prediction), axis=1)\n",
        "                if probabilities.item() > high_confidence_threshold:\n",
        "                    high_confidence_samples.append((text, predicted.item()))\n",
        "\n",
        "    if len(high_confidence_samples) == 0:\n",
        "        break\n",
        "\n",
        "    # 伪标签数据\n",
        "    X_pseudo_labeled, y_pseudo_labeled = zip(*high_confidence_samples)\n",
        "    X_pseudo_labeled = [prepare_sequence(text, dataset_agnews_word2vec) for text in X_pseudo_labeled]\n",
        "    y_pseudo_labeled = list(y_pseudo_labeled)\n",
        "\n",
        "    # 从未标签的数据池中移除伪标签的数据\n",
        "    dataset_agnews_processed_texts = [text for i, text in enumerate(dataset_agnews_processed_texts) if sentence_class_pseudolabels[i] == -1]\n",
        "    sentence_class_pseudolabels = [label for label in sentence_class_pseudolabels if label == -1]\n",
        "\n",
        "    # 将伪标签数据添加到训练集\n",
        "    train_data.extend(list(zip(X_pseudo_labeled, y_pseudo_labeled)))\n",
        "\n",
        "    # 使用增强的数据集重新创建DataLoader\n",
        "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    # 使用伪标签的数据重新训练模型\n",
        "    train(model, train_loader, test_loader, criterion, optimizer, epochs=5)\n"
      ],
      "metadata": {
        "id": "4jyTnhUOqmo0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}