{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1AQmlagiG5sxcanPH2BJm_V-qXaR0wKR7",
      "authorship_tag": "ABX9TyOb/LxYOgd9frqIZyK9DhfL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KFCFKXQS/math/blob/main/weak_supervision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "# 下载NLTK的相关资源\n",
        "nltk.download('punkt')  # 分词所需数据\n",
        "nltk.download('averaged_perceptron_tagger')  # 词性标注所需数据\n",
        "\n",
        "# 选择设备，如果有CUDA则使用CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JJmwoeDqmfc",
        "outputId": "96eaaa2f-a1f8-4c56-ebce-ee4e4f3d4238"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 1. glove_50d\n",
        "glove_50d={}\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/LSTM/glove.6B.50d.txt\") as f:\n",
        "    next(f)\n",
        "    for line in f:\n",
        "        line=line.split()\n",
        "        glove_50d[line[0]]=[float(xi) for xi in line[1:]]\n"
      ],
      "metadata": {
        "id": "Ye3lscANlXNg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## 2.1 dataset_agnews\n",
        "# a、读取文档\n",
        "def clean_text(sentence):\n",
        "    cleaned_sentence = re.sub(r'[^\\w\\s]', '', sentence)  # 去除标点符号和特殊字符\n",
        "    cleaned_sentence = cleaned_sentence.lower()  # 转换为小写\n",
        "    tokens = nltk.word_tokenize(cleaned_sentence)  # 分词\n",
        "    tagged_tokens = nltk.pos_tag(tokens)  # 词性标注\n",
        "    # 保留动词和名词\n",
        "    cleaned_tokens = [token for token, pos in tagged_tokens if pos.startswith('NN')]\n",
        "    cleaned_sentence = ' '.join(cleaned_tokens)  # 拼接为字符串\n",
        "    return cleaned_sentence\n",
        "\n",
        "dataset_agnews_texts=[]\n",
        "dataset_agnews_reallabels=[]\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/dataset/agnews/dataset.csv') as f:\n",
        "    for line in f:\n",
        "        dataset_agnews_reallabels.append(int(line[0]))\n",
        "        dataset_agnews_texts.append(clean_text(line[1:]).strip().split())\n"
      ],
      "metadata": {
        "id": "KXyA4xTymOf2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# b、读取已知类别和关键词\n",
        "dataset_agnews_classes={}\n",
        "dataset_agnews_keywords={}\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/dataset/agnews/classes.txt') as f:\n",
        "    for line in f:\n",
        "        line=line.strip().split(':')\n",
        "        dataset_agnews_classes[int(line[0])]=line[1]\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/dataset/agnews/keywords.txt') as f:\n",
        "    for line in f:\n",
        "        line = re.split(r\"[:,\\s]+\", line.strip())\n",
        "        dataset_agnews_keywords[int(line[0])]=set(line[1:])\n",
        "\n",
        "# c、构建词典和Word2Vec映射表.并将原文档中不在glove里的未知词改为<unk>\n",
        "dataset_agnews_words=set()\n",
        "dataset_agnews_word2vec={}\n",
        "dataset_agnews_vec2word={}\n",
        "dataset_agnews_processed_texts=[]\n",
        "\n",
        "unk_vector = [0] * 50\n",
        "for sentence in dataset_agnews_texts:\n",
        "    processed_sentence=[]\n",
        "    for word in sentence:\n",
        "        if word in glove_50d.keys():\n",
        "            dataset_agnews_words.add(word)\n",
        "            vec = glove_50d[word]\n",
        "            dataset_agnews_word2vec[word] = vec\n",
        "            dataset_agnews_vec2word[tuple(vec)] = word\n",
        "            processed_sentence.append(word)\n",
        "\n",
        "        else:\n",
        "            # 把不在glove50d的词 改成<unk>\n",
        "            dataset_agnews_words.add(\"<unk>\")\n",
        "            dataset_agnews_word2vec[\"<unk>\"] = unk_vector\n",
        "            dataset_agnews_vec2word[tuple(unk_vector)] = \"<unk>\"\n",
        "            processed_sentence.append(\"<unk>\")\n",
        "    dataset_agnews_processed_texts.append(processed_sentence)\n",
        "\n",
        "cosine_similarity_matrix=cosine_similarity([vec for vec in dataset_agnews_word2vec.values()])\n",
        "keys_list = list(dataset_agnews_word2vec.keys())"
      ],
      "metadata": {
        "id": "bgWDSFdDmTjd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(dataset_agnews_word2vec.items())[0])\n",
        "print(list(dataset_agnews_word2vec.items())[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiIzQykEm1Oo",
        "outputId": "20e6eaa6-aaaa-4482-e550-ff08ae02b6c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('wall', [0.26382, 0.32453, 0.74185, -0.37095, 0.65957, -0.49222, -0.55538, -0.23779, -0.44918, -0.12702, -0.86794, -0.4006, -0.80488, 0.48755, -0.18839, 0.53307, -0.23213, -1.2418, -0.34996, -0.80586, 0.65294, -0.49259, -0.8745, -0.81071, -0.087246, -1.2377, -0.65882, 1.1209, 0.13363, -0.23701, 3.0263, -0.71435, 1.4986, -0.033124, -1.0149, -0.15854, -0.040294, -0.17169, 0.58463, -0.63653, -0.062352, -0.078485, -0.16274, 0.5391, 0.78765, -0.095975, 0.30811, -0.77773, 0.16744, -0.81749])\n",
            "('st', [0.64859, 2.4722, -0.64446, -1.114, 0.23142, 0.019663, -0.91858, 0.19075, -0.19415, -0.49484, 0.23414, 0.73106, -0.61235, -1.2222, -0.93782, 0.1332, -0.35044, -0.96254, -1.2712, 0.44081, -0.11185, 0.1422, -0.80163, 0.46084, -0.43391, -0.28229, 0.030046, -0.53431, -1.0732, 0.40196, 1.6818, 0.47278, 1.0622, -0.38899, 0.59502, -0.37821, 1.1789, 0.071788, 0.82684, 0.22042, 0.75696, -0.39883, -0.29256, -0.065231, -0.23903, 1.7483, -0.74774, -1.522, 0.59868, -0.56331])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 扩充关键词\n",
        "for label in dataset_agnews_keywords.keys():\n",
        "    expanded_keywords = set(dataset_agnews_keywords[label])\n",
        "    for word in dataset_agnews_keywords[label]:\n",
        "        word_index = keys_list.index(word)\n",
        "        similar_indices = np.where(cosine_similarity_matrix[word_index] > 0.8)[0]\n",
        "        for similar_index in similar_indices:\n",
        "            similar_word = keys_list[similar_index]\n",
        "            expanded_keywords.add(similar_word)\n",
        "    dataset_agnews_keywords[label] = expanded_keywords\n"
      ],
      "metadata": {
        "id": "RUK6azfvl1kO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算每个句子的类别频率\n",
        "sentence_class_freq = []\n",
        "for sentence in dataset_agnews_processed_texts:\n",
        "    freq = [0]*len(dataset_agnews_classes)\n",
        "    for word in sentence:\n",
        "        for key, keywords in dataset_agnews_keywords.items():\n",
        "            if word in keywords:\n",
        "                freq[key] += 1\n",
        "    sentence_class_freq.append(freq)\n",
        "\n",
        "# 根据频率赋予句子类别标签\n",
        "sentence_class_pseudolabels = []\n",
        "for freq in sentence_class_freq:\n",
        "    max_index = np.argmax(freq)\n",
        "    max_value = freq[max_index]\n",
        "    sum_other_frequencies = sum(f for i, f in enumerate(freq) if i != max_index)\n",
        "    if max_value >2 * sum_other_frequencies:\n",
        "        sentence_class_pseudolabels.append(max_index)\n",
        "    else:\n",
        "        sentence_class_pseudolabels.append(-1)\n"
      ],
      "metadata": {
        "id": "XihFv4eompEo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算准确率（用于评估关键词选择算法）\n",
        "correct_classified=0\n",
        "total_classified=0\n",
        "\n",
        "for assigned_label, real_label in zip(sentence_class_pseudolabels, dataset_agnews_reallabels):\n",
        "    if assigned_label != -1:\n",
        "        total_classified += 1\n",
        "        if assigned_label == real_label:\n",
        "            correct_classified += 1\n",
        "\n",
        "print(\"Total classified: \", total_classified)\n",
        "print(\"Correct classified: \", correct_classified)\n",
        "print(\"Accuracy: \", correct_classified / total_classified)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxWOaX2CnbOa",
        "outputId": "447ceb53-f2af-4406-8810-7255e4a37833"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total classified:  14794\n",
            "Correct classified:  11535\n",
            "Accuracy:  0.7797079897255644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 把每句话的词转化为vec\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return idxs\n",
        "\n",
        "inputs = [prepare_sequence(sent,dataset_agnews_word2vec) for sent in dataset_agnews_processed_texts]\n"
      ],
      "metadata": {
        "id": "79Djd44_ne9d"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(inputs[1])):\n",
        "  print(dataset_agnews_vec2word[tuple(inputs[1][i])])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GJX__sOHcP2",
        "outputId": "f649e754-c4f1-4357-877d-363eeef44a85"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aerospace\n",
            "investment\n",
            "firm\n",
            "carlyle\n",
            "group\n",
            "reputation\n",
            "plays\n",
            "defense\n",
            "industry\n",
            "bets\n",
            "part\n",
            "market\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert (len(sentence_class_pseudolabels)==len(inputs)),'not compare'"
      ],
      "metadata": {
        "id": "9bBL9ZoWI07d"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 前面标记-1的表示未打标签，去掉它们\n",
        "origin_training_texts=[]\n",
        "origin_training_labels=[]\n",
        "for i in range(len(inputs)):\n",
        "  if sentence_class_pseudolabels[i]!=-1:\n",
        "    origin_training_texts.append(list(inputs[i]))\n",
        "    origin_training_labels.append(list(sentence_class_pseudolabels)[i])"
      ],
      "metadata": {
        "id": "l8YefI7LtGWw"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_class_pseudolabels[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxjlNQwHI-_1",
        "outputId": "6a7f6a12-d3ae-49a7-cb48-2a412c75eb09"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1, 2, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5YU5IUdjlTLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert (origin_training_texts[0]==inputs[1]),'a'\n",
        "print(len(origin_training_texts[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVj_UuBCIIK4",
        "outputId": "b228a09d-9e88-42ea-fb33-fc716ae8749b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 50\n",
        "# 隐藏层维度64\n",
        "hidden_size = 64\n",
        "# 输出的维度4\n",
        "output_size = len(list(dataset_agnews_classes.keys()))\n"
      ],
      "metadata": {
        "id": "w7jq0LrAnYzk"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义模型\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.hidden2out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        packed_x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True)\n",
        "        packed_output, _ = self.lstm(packed_x)\n",
        "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "        output = self.hidden2out(output[:, -1, :])\n",
        "        output = self.softmax(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# 构建模型\n",
        "model = LSTMClassifier(input_size, hidden_size, output_size).to(device)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05)\n"
      ],
      "metadata": {
        "id": "9Hivvn6qn3lA"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # 对输入序列进行填充\n",
        "    sorted_batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
        "    sequences = [torch.tensor(x[0]) for x in sorted_batch]\n",
        "    sequences_padded = pad_sequence(sequences, batch_first=True)\n",
        "    lengths = torch.LongTensor([len(x[0]) for x in sorted_batch])\n",
        "    labels = torch.LongTensor([x[1] for x in sorted_batch])\n",
        "    return sequences_padded, labels, lengths\n",
        "\n",
        "\n",
        "# 原始数据和标签\n",
        "origin_data = list(zip(origin_training_texts, origin_training_labels))\n",
        "\n",
        "# 划分训练集和测试集\n",
        "train_data, test_data = train_test_split(origin_data, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "DD665uMYukew"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(train_data[0][0])):\n",
        "  print(dataset_agnews_vec2word[tuple(train_data[0][0][i])])\n",
        "\n",
        "print(len(train_data),len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nu-o793LxaJ",
        "outputId": "3a236c77-881c-4f99-e3f4-f67d8145bef2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rookie\n",
            "kazmir\n",
            "outduels\n",
            "confidence\n",
            "environment\n",
            "baseball\n",
            "pitcher\n",
            "sense\n",
            "wonder\n",
            "11835 2959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建DataLoader\n",
        "train_loader = DataLoader(train_data, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "p2D07xCQLpa-"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "for batch in train_loader:\n",
        "    if i>=1:\n",
        "      break\n",
        "    i+=1\n",
        "    sequences_padded, labels, lengths = batch\n",
        "    # 打印每个批次的数据\n",
        "    # 精度变化导致不能直接作为key查找vec2word，搜最接近的词向量\n",
        "    for i in range(len(sequences_padded[0])):\n",
        "        min_distance = float('inf')  # 初始化最小距离为正无穷\n",
        "        closest_word = None\n",
        "\n",
        "        for word_vector in dataset_agnews_vec2word.keys():\n",
        "            distance = np.linalg.norm(np.array(sequences_padded[0][i]) - np.array(word_vector))  # 计算欧几里德距离\n",
        "            if distance < min_distance:\n",
        "                min_distance = distance\n",
        "                closest_word = tuple(word_vector)\n",
        "\n",
        "        print(\"Closest Word:\", dataset_agnews_vec2word[closest_word])\n",
        "    #print(\"Sequences Padded:\", sequences_padded[0])\n",
        "    print(\"Labels:\", labels)\n",
        "    print(\"Lengths:\", lengths)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB2IK-SwPkos",
        "outputId": "d2727c8a-03d2-4879-dee2-c15093658c79"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closest Word: cell\n",
            "Closest Word: phone\n",
            "Closest Word: maker\n",
            "Closest Word: year\n",
            "Closest Word: s\n",
            "Closest Word: reports\n",
            "Closest Word: batteries\n",
            "Closest Word: phones\n",
            "Closest Word: concern\n",
            "Closest Word: radiation\n",
            "Closest Word: devices\n",
            "Closest Word: siemens\n",
            "Closest Word: software\n",
            "Closest Word: defect\n",
            "Closest Word: range\n",
            "Closest Word: phones\n",
            "Labels: tensor([3])\n",
            "Lengths: tensor([16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, test_loader, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_inputs, batch_labels, batch_lengths in train_loader:\n",
        "            batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)\n",
        "            # Keep lengths on CPU\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_inputs, batch_lengths)\n",
        "            loss = criterion(output, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "        print(f'After Epoch {epoch + 1}/{epochs} Loss: {loss.item()} Test Loss: {test_loss} Test Acc: {test_acc}')\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_inputs, batch_labels, batch_lengths in test_loader:\n",
        "            batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)\n",
        "            # Keep lengths on CPU\n",
        "            output = model(batch_inputs, batch_lengths)\n",
        "            loss = criterion(output, batch_labels)\n",
        "            total_loss += loss.item() * batch_inputs.size(0)\n",
        "            predicted = torch.argmax(output, axis=1)\n",
        "            correct = (predicted == batch_labels).sum().item()\n",
        "            total_correct += correct\n",
        "            total_count += batch_inputs.size(0)\n",
        "\n",
        "    # 计算平均损失和精度\n",
        "    avg_loss = total_loss / total_count\n",
        "    accuracy = total_correct / total_count\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "\n",
        "# 开始训练\n",
        "train(model, train_loader, test_loader, criterion, optimizer, epochs=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "I5RyDaXCu-4M",
        "outputId": "555e34f0-7814-467e-a6af-49af9682a7f9"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 Loss: 2.264974000354414e-06 Test Loss: 0.015330512642520106 Test Acc: 0.994930719837783\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-4539409e6d0d>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# 开始训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-4539409e6d0d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 自学习阶段\n",
        "high_confidence_threshold = 0.9\n",
        "\n",
        "while True:\n",
        "    high_confidence_samples = []\n",
        "    high_confidence_indices = [] # 保存高置信度样本id\n",
        "\n",
        "    for i, text in enumerate(dataset_agnews_processed_texts):\n",
        "        if sentence_class_pseudolabels[i] == -1:  # 未标签的\n",
        "            with torch.no_grad():\n",
        "                text_tensor = torch.tensor([prepare_sequence(text, dataset_agnews_word2vec)]).float().to(device)\n",
        "                prediction = model(text_tensor, [len(text)])\n",
        "                probabilities, predicted = torch.max(torch.exp(prediction), axis=1)\n",
        "                if probabilities.item() > high_confidence_threshold:\n",
        "                    high_confidence_samples.append((text, predicted.item()))\n",
        "                    high_confidence_indices.append(i) # 保存索引\n",
        "\n",
        "    if len(high_confidence_samples) == 0:\n",
        "        break\n",
        "    print('rest', len(high_confidence_samples))\n",
        "\n",
        "    # 更新伪标签\n",
        "    for index in high_confidence_indices:\n",
        "        sentence_class_pseudolabels[index] = 1 # 或者任何非-1的值，以表示这个样本已经被标签化了\n",
        "\n",
        "    # 伪标签数据\n",
        "    X_pseudo_labeled, y_pseudo_labeled = zip(*high_confidence_samples)\n",
        "    X_pseudo_labeled = [prepare_sequence(text, dataset_agnews_word2vec) for text in X_pseudo_labeled]\n",
        "    y_pseudo_labeled = list(y_pseudo_labeled)\n",
        "\n",
        "    # 从未标签的数据池中移除伪标签的数据\n",
        "    dataset_agnews_processed_texts = [text for i, text in enumerate(dataset_agnews_processed_texts) if sentence_class_pseudolabels[i] == -1]\n",
        "    sentence_class_pseudolabels = [label for label in sentence_class_pseudolabels if label == -1]\n",
        "\n",
        "    # 将伪标签数据添加到训练集\n",
        "    train_data.extend(list(zip(X_pseudo_labeled, y_pseudo_labeled)))\n",
        "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    # 重新训练模型\n",
        "    train(model, train_loader, test_loader, criterion, optimizer, epochs=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4jyTnhUOqmo0",
        "outputId": "4e3d9600-2dae-4f15-9891-ab1ba2da13a8"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rest 17397\n",
            "Epoch 1/5 Loss: 1.2688857316970825 Test Loss: 0.013609577720462116 Test Acc: 0.9942548158161542\n",
            "Epoch 2/5 Loss: 1.341914415359497 Test Loss: 0.013663483065301235 Test Acc: 0.9942548158161542\n",
            "Epoch 3/5 Loss: 1.2029669284820557 Test Loss: 0.014560110023546423 Test Acc: 0.9942548158161542\n",
            "Epoch 4/5 Loss: 1.2604386806488037 Test Loss: 0.014588519719128994 Test Acc: 0.9942548158161542\n",
            "Epoch 5/5 Loss: 1.2512760162353516 Test Loss: 0.01467829977687649 Test Acc: 0.9942548158161542\n",
            "rest 424\n",
            "Epoch 1/5 Loss: 1.1459002494812012 Test Loss: 0.014878939543280375 Test Acc: 0.9932409597837107\n",
            "Epoch 2/5 Loss: 1.1532992124557495 Test Loss: 0.015181252793088584 Test Acc: 0.9932409597837107\n",
            "Epoch 3/5 Loss: 1.1518064737319946 Test Loss: 0.015317580952003298 Test Acc: 0.9939168638053396\n",
            "Epoch 4/5 Loss: 1.0419443845748901 Test Loss: 0.015190526503364525 Test Acc: 0.9939168638053396\n",
            "Epoch 5/5 Loss: 1.0992405414581299 Test Loss: 0.015728231827660248 Test Acc: 0.9925650557620818\n",
            "rest 334\n",
            "Epoch 1/5 Loss: 1.1190639734268188 Test Loss: 0.01592673343336091 Test Acc: 0.9925650557620818\n",
            "Epoch 2/5 Loss: 1.1627026796340942 Test Loss: 0.01595042032915332 Test Acc: 0.9925650557620818\n",
            "Epoch 3/5 Loss: 1.092871904373169 Test Loss: 0.01605404007367763 Test Acc: 0.9925650557620818\n",
            "Epoch 4/5 Loss: 1.2531371116638184 Test Loss: 0.016432599906764753 Test Acc: 0.9929030077728962\n",
            "Epoch 5/5 Loss: 1.384069800376892 Test Loss: 0.016746041941067442 Test Acc: 0.9925650557620818\n",
            "rest 304\n",
            "Epoch 1/5 Loss: 0.9385280013084412 Test Loss: 0.01649907667740683 Test Acc: 0.9929030077728962\n",
            "Epoch 2/5 Loss: 1.0044649839401245 Test Loss: 0.016841007938753098 Test Acc: 0.9922271037512673\n",
            "Epoch 3/5 Loss: 1.0654436349868774 Test Loss: 0.01732411865234503 Test Acc: 0.9922271037512673\n",
            "Epoch 4/5 Loss: 0.983277440071106 Test Loss: 0.01758744500204537 Test Acc: 0.9918891517404529\n",
            "Epoch 5/5 Loss: 1.1928765773773193 Test Loss: 0.01685538203951273 Test Acc: 0.9922271037512673\n",
            "rest 216\n",
            "Epoch 1/5 Loss: 1.2674369812011719 Test Loss: 0.016802500582493773 Test Acc: 0.9925650557620818\n",
            "Epoch 2/5 Loss: 1.1746470928192139 Test Loss: 0.01733530351880284 Test Acc: 0.9915511997296383\n",
            "Epoch 3/5 Loss: 1.1672558784484863 Test Loss: 0.01781342985095814 Test Acc: 0.9915511997296383\n",
            "Epoch 4/5 Loss: 1.1836624145507812 Test Loss: 0.017734774222382763 Test Acc: 0.9915511997296383\n",
            "Epoch 5/5 Loss: 1.1718134880065918 Test Loss: 0.01797148425495773 Test Acc: 0.9915511997296383\n",
            "rest 134\n",
            "Epoch 1/5 Loss: 1.2500636577606201 Test Loss: 0.017953661589788644 Test Acc: 0.9915511997296383\n",
            "Epoch 2/5 Loss: 1.2924306392669678 Test Loss: 0.0179858416367187 Test Acc: 0.9912132477188239\n",
            "Epoch 3/5 Loss: 1.2190182209014893 Test Loss: 0.01822637813189074 Test Acc: 0.9912132477188239\n",
            "Epoch 4/5 Loss: 1.2827401161193848 Test Loss: 0.01806663248940074 Test Acc: 0.9918891517404529\n",
            "Epoch 5/5 Loss: 1.4092700481414795 Test Loss: 0.018193626493991397 Test Acc: 0.9915511997296383\n",
            "rest 112\n",
            "Epoch 1/5 Loss: 1.117482304573059 Test Loss: 0.018059203098520347 Test Acc: 0.9915511997296383\n",
            "Epoch 2/5 Loss: 1.232615351676941 Test Loss: 0.01793655355980897 Test Acc: 0.9918891517404529\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-fc632cc3bc87>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# 使用伪标签的数据重新训练模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-4539409e6d0d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "predicted_labels = []\n",
        "for sentence in inputs:\n",
        "  sentence = torch.tensor(sentence).to(device)\n",
        "  a = model(sentence.unsqueeze(0), lengths=[len(sentence)])\n",
        "  _, predicted = torch.max(a, 1)  # 获取预测的类别\n",
        "  predicted_labels.extend(predicted.cpu().numpy())  # 将预测的标签加到列表\n",
        "\n",
        "# 计算acc和F1分\n",
        "acc = accuracy_score(dataset_agnews_reallabels, predicted_labels)\n",
        "f1 = f1_score(dataset_agnews_reallabels, predicted_labels, average='macro')  # 或者 'micro'\n",
        "\n",
        "print('Accuracy:', acc)\n",
        "print('F1 Score:', f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-PmTafrdPB6",
        "outputId": "6410372c-1174-48cd-bb94-752440d64ecd"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.73135\n",
            "F1 Score: 0.7280739136087936\n"
          ]
        }
      ]
    }
  ]
}