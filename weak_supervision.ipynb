{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1AQmlagiG5sxcanPH2BJm_V-qXaR0wKR7",
      "authorship_tag": "ABX9TyPI4ikgrKf1V8GBvLCiGUwJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KFCFKXQS/math/blob/main/weak_supervision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "# 下载NLTK的相关资源\n",
        "nltk.download('punkt')  # 分词所需数据\n",
        "nltk.download('averaged_perceptron_tagger')  # 词性标注所需数据\n",
        "\n",
        "# 选择设备，如果有CUDA则使用CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JJmwoeDqmfc",
        "outputId": "c3ea7d77-9516-4e1a-ae8a-7c581dce3b4b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 1. glove_50d\n",
        "glove_50d={}\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/LSTM/glove.6B.50d.txt\") as f:\n",
        "    next(f)\n",
        "    for line in f:\n",
        "        line=line.split()\n",
        "        glove_50d[line[0]]=[float(xi) for xi in line[1:]]\n"
      ],
      "metadata": {
        "id": "Ye3lscANlXNg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## 2.1 dataset_agnews\n",
        "# a、读取文档\n",
        "def clean_text(sentence):\n",
        "    cleaned_sentence = re.sub(r'[^\\w\\s]', '', sentence)  # 去除标点符号和特殊字符\n",
        "    cleaned_sentence = cleaned_sentence.lower()  # 转换为小写\n",
        "    tokens = nltk.word_tokenize(cleaned_sentence)  # 分词\n",
        "    tagged_tokens = nltk.pos_tag(tokens)  # 词性标注\n",
        "    # 保留动词和名词\n",
        "    cleaned_tokens = [token for token, pos in tagged_tokens if pos.startswith('NN')]\n",
        "    cleaned_sentence = ' '.join(cleaned_tokens)  # 拼接为字符串\n",
        "    return cleaned_sentence\n",
        "\n",
        "dataset_agnews_texts=[]\n",
        "dataset_agnews_reallabels=[]\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/dataset/agnews/dataset.csv') as f:\n",
        "    for line in f:\n",
        "        dataset_agnews_reallabels.append(int(line[0]))\n",
        "        dataset_agnews_texts.append(clean_text(line[1:]).strip().split())\n"
      ],
      "metadata": {
        "id": "KXyA4xTymOf2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# b、读取已知类别和关键词\n",
        "dataset_agnews_classes={}\n",
        "dataset_agnews_keywords={}\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/dataset/agnews/classes.txt') as f:\n",
        "    for line in f:\n",
        "        line=line.strip().split(':')\n",
        "        dataset_agnews_classes[int(line[0])]=line[1]\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/dataset/agnews/keywords.txt') as f:\n",
        "    for line in f:\n",
        "        line = re.split(r\"[:,\\s]+\", line.strip())\n",
        "        dataset_agnews_keywords[int(line[0])]=set(line[1:])\n",
        "\n",
        "# c、构建词典和Word2Vec映射表.并将原文档中不在glove里的未知词改为<unk>\n",
        "dataset_agnews_words=set()\n",
        "dataset_agnews_word2vec={}\n",
        "dataset_agnews_vec2word={}\n",
        "dataset_agnews_processed_texts=[]\n",
        "\n",
        "unk_vector = [0] * 50\n",
        "for sentence in dataset_agnews_texts:\n",
        "    processed_sentence=[]\n",
        "    for word in sentence:\n",
        "        if word in glove_50d.keys():\n",
        "            dataset_agnews_words.add(word)\n",
        "            vec = glove_50d[word]\n",
        "            dataset_agnews_word2vec[word] = vec\n",
        "            dataset_agnews_vec2word[tuple(vec)] = word\n",
        "            processed_sentence.append(word)\n",
        "\n",
        "        else:\n",
        "            # 把不在glove50d的词 改成<unk>\n",
        "            dataset_agnews_words.add(\"<unk>\")\n",
        "            dataset_agnews_word2vec[\"<unk>\"] = unk_vector\n",
        "            dataset_agnews_vec2word[tuple(unk_vector)] = \"<unk>\"\n",
        "            processed_sentence.append(\"<unk>\")\n",
        "    dataset_agnews_processed_texts.append(processed_sentence)\n",
        "\n",
        "cosine_similarity_matrix=cosine_similarity([vec for vec in dataset_agnews_word2vec.values()])\n",
        "keys_list = list(dataset_agnews_word2vec.keys())"
      ],
      "metadata": {
        "id": "bgWDSFdDmTjd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(dataset_agnews_word2vec.items())[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiIzQykEm1Oo",
        "outputId": "76a4f844-c32a-4b47-894a-8ddeb4e7f77e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('wall', [0.26382, 0.32453, 0.74185, -0.37095, 0.65957, -0.49222, -0.55538, -0.23779, -0.44918, -0.12702, -0.86794, -0.4006, -0.80488, 0.48755, -0.18839, 0.53307, -0.23213, -1.2418, -0.34996, -0.80586, 0.65294, -0.49259, -0.8745, -0.81071, -0.087246, -1.2377, -0.65882, 1.1209, 0.13363, -0.23701, 3.0263, -0.71435, 1.4986, -0.033124, -1.0149, -0.15854, -0.040294, -0.17169, 0.58463, -0.63653, -0.062352, -0.078485, -0.16274, 0.5391, 0.78765, -0.095975, 0.30811, -0.77773, 0.16744, -0.81749]), ('st', [0.64859, 2.4722, -0.64446, -1.114, 0.23142, 0.019663, -0.91858, 0.19075, -0.19415, -0.49484, 0.23414, 0.73106, -0.61235, -1.2222, -0.93782, 0.1332, -0.35044, -0.96254, -1.2712, 0.44081, -0.11185, 0.1422, -0.80163, 0.46084, -0.43391, -0.28229, 0.030046, -0.53431, -1.0732, 0.40196, 1.6818, 0.47278, 1.0622, -0.38899, 0.59502, -0.37821, 1.1789, 0.071788, 0.82684, 0.22042, 0.75696, -0.39883, -0.29256, -0.065231, -0.23903, 1.7483, -0.74774, -1.522, 0.59868, -0.56331]), ('bears', [-0.34941, 0.87778, -0.05669, -0.13139, 0.90058, 0.55142, -1.2957, -0.26037, -0.26368, -0.43959, 0.0030158, 0.19821, 0.53337, -0.24056, 0.22478, 0.3003, 0.37854, -0.15807, -1.3868, 0.64066, -1.2505, -1.0045, 0.40487, -0.58626, -0.10931, -1.1765, -0.69832, 0.73902, 0.74936, -1.392, 1.3813, 0.51727, 0.33017, -0.82468, 0.28004, 0.74484, -0.0097628, -0.12186, -0.2318, -0.59191, -0.70521, -0.42016, -0.32745, -0.033317, 0.20024, 0.26211, -0.07747, 0.44474, 0.061639, -0.33998])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 扩充关键词\n",
        "for label in dataset_agnews_keywords.keys():\n",
        "    expanded_keywords = set(dataset_agnews_keywords[label])\n",
        "    for word in dataset_agnews_keywords[label]:\n",
        "        word_index = keys_list.index(word)\n",
        "        similar_indices = np.where(cosine_similarity_matrix[word_index] > 0.8)[0]\n",
        "        for similar_index in similar_indices:\n",
        "            similar_word = keys_list[similar_index]\n",
        "            expanded_keywords.add(similar_word)\n",
        "    dataset_agnews_keywords[label] = expanded_keywords\n"
      ],
      "metadata": {
        "id": "RUK6azfvl1kO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 计算每个句子的类别频率\n",
        "sentence_class_freq = []\n",
        "for sentence in dataset_agnews_processed_texts:\n",
        "    freq = [0]*len(dataset_agnews_classes)\n",
        "    for word in sentence:\n",
        "        for key, keywords in dataset_agnews_keywords.items():\n",
        "            if word in keywords:\n",
        "                freq[key] += 1\n",
        "    sentence_class_freq.append(freq)\n",
        "\n",
        "# 根据频率赋予句子类别标签\n",
        "sentence_class_pseudolabels = []\n",
        "for freq in sentence_class_freq:\n",
        "    max_index = np.argmax(freq)\n",
        "    max_value = freq[max_index]\n",
        "    sum_other_frequencies = sum(f for i, f in enumerate(freq) if i != max_index)\n",
        "    if max_value >2 * sum_other_frequencies:\n",
        "        sentence_class_pseudolabels.append(max_index)\n",
        "    else:\n",
        "        sentence_class_pseudolabels.append(-1)\n"
      ],
      "metadata": {
        "id": "XihFv4eompEo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 计算准确率\n",
        "correct_classified=0\n",
        "total_classified=0\n",
        "\n",
        "for assigned_label, real_label in zip(sentence_class_pseudolabels, dataset_agnews_reallabels):\n",
        "    if assigned_label != -1:\n",
        "        total_classified += 1\n",
        "        if assigned_label == real_label:\n",
        "            correct_classified += 1\n",
        "\n",
        "print(\"Total classified: \", total_classified)\n",
        "print(\"Correct classified: \", correct_classified)\n",
        "print(\"Accuracy: \", correct_classified / total_classified)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxWOaX2CnbOa",
        "outputId": "c7dfa72f-bff9-4fcc-ae7b-b7d56591675f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total classified:  14794\n",
            "Correct classified:  11535\n",
            "Accuracy:  0.7797079897255644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 把每句话的词转化为数字编码\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return idxs\n",
        "\n",
        "inputs = [prepare_sequence(sent,dataset_agnews_word2vec) for sent in dataset_agnews_processed_texts]\n"
      ],
      "metadata": {
        "id": "79Djd44_ne9d"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 前面标记-1的表示未打标签，去掉它们\n",
        "origin_training_texts=[]\n",
        "origin_training_labels=[]\n",
        "for i in range(len(inputs)):\n",
        "  if sentence_class_pseudolabels[i]!=-1:\n",
        "    origin_training_texts.append(list(inputs[i]))\n",
        "    origin_training_labels.append(list(sentence_class_pseudolabels)[i])"
      ],
      "metadata": {
        "id": "l8YefI7LtGWw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pIwm5b8n3exH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 50\n",
        "# 隐藏层的维度为64\n",
        "hidden_size = 64\n",
        "# 输出的维度4\n",
        "output_size = len(list(dataset_agnews_classes.keys()))\n"
      ],
      "metadata": {
        "id": "w7jq0LrAnYzk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义模型\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.hidden2out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "\n",
        "        packed_x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True)\n",
        "        packed_output, _ = self.lstm(packed_x)\n",
        "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "        output = self.hidden2out(output[:, -1, :])\n",
        "        output = self.softmax(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# 构建模型\n",
        "model = LSTMClassifier(input_size, hidden_size, output_size).to(device)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05)\n"
      ],
      "metadata": {
        "id": "9Hivvn6qn3lA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # 对输入序列进行填充\n",
        "    sorted_batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
        "    sequences = [torch.tensor(x[0]) for x in sorted_batch]\n",
        "    sequences_padded = pad_sequence(sequences, batch_first=True)\n",
        "    lengths = torch.LongTensor([len(x[0]) for x in sorted_batch])\n",
        "    labels = torch.LongTensor([x[1] for x in sorted_batch])\n",
        "    return sequences_padded, labels, lengths\n",
        "\n",
        "\n",
        "# 原始数据和标签\n",
        "origin_data = list(zip(origin_training_texts, origin_training_labels))\n",
        "\n",
        "# 划分训练集和测试集\n",
        "train_data, test_data = train_test_split(origin_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# 创建DataLoader\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
        "\n"
      ],
      "metadata": {
        "id": "DD665uMYukew"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, test_loader, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_inputs, batch_labels, batch_lengths in train_loader:\n",
        "            batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)\n",
        "            # Keep lengths on CPU\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_inputs, batch_lengths)\n",
        "            loss = criterion(output, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "        print(f'Epoch {epoch + 1}/{epochs} Loss: {loss.item()} Test Loss: {test_loss} Test Acc: {test_acc}')\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_inputs, batch_labels, batch_lengths in test_loader:\n",
        "            batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)\n",
        "            # Keep lengths on CPU\n",
        "            output = model(batch_inputs, batch_lengths)\n",
        "            loss = criterion(output, batch_labels)\n",
        "            total_loss += loss.item() * batch_inputs.size(0)\n",
        "            predicted = torch.argmax(output, axis=1)\n",
        "            correct = (predicted == batch_labels).sum().item()\n",
        "            total_correct += correct\n",
        "            total_count += batch_inputs.size(0)\n",
        "\n",
        "    # 计算平均损失和精度\n",
        "    avg_loss = total_loss / total_count\n",
        "    accuracy = total_correct / total_count\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "\n",
        "# 开始训练\n",
        "train(model, train_loader, test_loader, criterion, optimizer, epochs=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I5RyDaXCu-4M",
        "outputId": "93d76fe9-af76-494e-da23-e99f2f8d52df"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 Loss: 1.3255268335342407 Test Loss: 1.3673081962028522 Test Acc: 0.32105441027374115\n",
            "Epoch 2/100 Loss: 1.422743558883667 Test Loss: 1.3658901293078722 Test Acc: 0.32679959445758705\n",
            "Epoch 3/100 Loss: 1.3469586372375488 Test Loss: 1.3653037614941637 Test Acc: 0.32848935451165934\n",
            "Epoch 4/100 Loss: 1.402023196220398 Test Loss: 1.3649049394233035 Test Acc: 0.3281514025008449\n",
            "Epoch 5/100 Loss: 1.3424988985061646 Test Loss: 1.3642396450687317 Test Acc: 0.32848935451165934\n",
            "Epoch 6/100 Loss: 1.36738920211792 Test Loss: 1.3636983347003546 Test Acc: 0.32848935451165934\n",
            "Epoch 7/100 Loss: 1.3678536415100098 Test Loss: 1.3633923924988531 Test Acc: 0.32848935451165934\n",
            "Epoch 8/100 Loss: 1.328890085220337 Test Loss: 1.3629593900346644 Test Acc: 0.32848935451165934\n",
            "Epoch 9/100 Loss: 1.3498376607894897 Test Loss: 1.3627372884637563 Test Acc: 0.32848935451165934\n",
            "Epoch 10/100 Loss: 1.3527532815933228 Test Loss: 1.3623677165413999 Test Acc: 0.32848935451165934\n",
            "Epoch 11/100 Loss: 1.403434157371521 Test Loss: 1.3623114502804632 Test Acc: 0.32848935451165934\n",
            "Epoch 12/100 Loss: 1.356899619102478 Test Loss: 1.3619183066649467 Test Acc: 0.32848935451165934\n",
            "Epoch 13/100 Loss: 1.3841594457626343 Test Loss: 1.3618266000744457 Test Acc: 0.32848935451165934\n",
            "Epoch 14/100 Loss: 1.3769701719284058 Test Loss: 1.3616881842305428 Test Acc: 0.32848935451165934\n",
            "Epoch 15/100 Loss: 1.3858290910720825 Test Loss: 1.3614396665014903 Test Acc: 0.32848935451165934\n",
            "Epoch 16/100 Loss: 1.3710885047912598 Test Loss: 1.361544552023405 Test Acc: 0.32848935451165934\n",
            "Epoch 17/100 Loss: 1.3128262758255005 Test Loss: 1.3612476738371855 Test Acc: 0.32848935451165934\n",
            "Epoch 18/100 Loss: 1.3394334316253662 Test Loss: 1.3614141721909172 Test Acc: 0.32848935451165934\n",
            "Epoch 19/100 Loss: 1.3045634031295776 Test Loss: 1.361232621841392 Test Acc: 0.32848935451165934\n",
            "Epoch 20/100 Loss: 1.3795063495635986 Test Loss: 1.3612669009379175 Test Acc: 0.32848935451165934\n",
            "Epoch 21/100 Loss: 1.3686248064041138 Test Loss: 1.3612678932071982 Test Acc: 0.32848935451165934\n",
            "Epoch 22/100 Loss: 1.3288871049880981 Test Loss: 1.3610036003706463 Test Acc: 0.32848935451165934\n",
            "Epoch 23/100 Loss: 1.363006830215454 Test Loss: 1.3609127189388708 Test Acc: 0.32848935451165934\n",
            "Epoch 24/100 Loss: 1.3541359901428223 Test Loss: 1.3606479426983766 Test Acc: 0.32848935451165934\n",
            "Epoch 25/100 Loss: 1.3614459037780762 Test Loss: 1.3606784983074476 Test Acc: 0.32848935451165934\n",
            "Epoch 26/100 Loss: 1.3753117322921753 Test Loss: 1.3606532677962757 Test Acc: 0.32848935451165934\n",
            "Epoch 27/100 Loss: 1.3395832777023315 Test Loss: 1.3608253707592453 Test Acc: 0.32848935451165934\n",
            "Epoch 28/100 Loss: 1.4077047109603882 Test Loss: 1.3605139350198177 Test Acc: 0.32848935451165934\n",
            "Epoch 29/100 Loss: 1.325190544128418 Test Loss: 1.3603373066562783 Test Acc: 0.32848935451165934\n",
            "Epoch 30/100 Loss: 1.332924246788025 Test Loss: 1.3601906302814026 Test Acc: 0.32848935451165934\n",
            "Epoch 31/100 Loss: 1.3674088716506958 Test Loss: 1.3602318703705252 Test Acc: 0.32848935451165934\n",
            "Epoch 32/100 Loss: 1.373620629310608 Test Loss: 1.360107818097833 Test Acc: 0.32848935451165934\n",
            "Epoch 33/100 Loss: 1.3177094459533691 Test Loss: 1.3600495722939252 Test Acc: 0.32882730652247383\n",
            "Epoch 34/100 Loss: 1.3622320890426636 Test Loss: 1.3597116361964832 Test Acc: 0.32882730652247383\n",
            "Epoch 35/100 Loss: 1.4127198457717896 Test Loss: 1.3596433222233424 Test Acc: 0.32882730652247383\n",
            "Epoch 36/100 Loss: 1.3571298122406006 Test Loss: 1.359723935984566 Test Acc: 0.32916525853328826\n",
            "Epoch 37/100 Loss: 1.3809040784835815 Test Loss: 1.3596453817360463 Test Acc: 0.32916525853328826\n",
            "Epoch 38/100 Loss: 1.319874882698059 Test Loss: 1.3595285178358871 Test Acc: 0.32916525853328826\n",
            "Epoch 39/100 Loss: 1.3540923595428467 Test Loss: 1.3592787649068288 Test Acc: 0.32916525853328826\n",
            "Epoch 40/100 Loss: 1.3509478569030762 Test Loss: 1.3589516905523225 Test Acc: 0.32916525853328826\n",
            "Epoch 41/100 Loss: 1.3660286664962769 Test Loss: 1.3589748572400793 Test Acc: 0.32916525853328826\n",
            "Epoch 42/100 Loss: 1.3636624813079834 Test Loss: 1.3589713561369705 Test Acc: 0.32916525853328826\n",
            "Epoch 43/100 Loss: 1.3921276330947876 Test Loss: 1.3587467349756002 Test Acc: 0.32916525853328826\n",
            "Epoch 44/100 Loss: 1.3515710830688477 Test Loss: 1.3580719359060445 Test Acc: 0.32950321054410275\n",
            "Epoch 45/100 Loss: 1.3196673393249512 Test Loss: 1.3580445833164279 Test Acc: 0.32950321054410275\n",
            "Epoch 46/100 Loss: 1.3387064933776855 Test Loss: 1.358193490786422 Test Acc: 0.32950321054410275\n",
            "Epoch 47/100 Loss: 1.3473368883132935 Test Loss: 1.3577761785852704 Test Acc: 0.32950321054410275\n",
            "Epoch 48/100 Loss: 1.3480761051177979 Test Loss: 1.3574372909084371 Test Acc: 0.32950321054410275\n",
            "Epoch 49/100 Loss: 1.3629213571548462 Test Loss: 1.3570014155769154 Test Acc: 0.3298411625549172\n",
            "Epoch 50/100 Loss: 1.3679049015045166 Test Loss: 1.3570064344531823 Test Acc: 0.3305170665765461\n",
            "Epoch 51/100 Loss: 1.3589496612548828 Test Loss: 1.3568286231821878 Test Acc: 0.33119297059817504\n",
            "Epoch 52/100 Loss: 1.366097331047058 Test Loss: 1.356488842704724 Test Acc: 0.33119297059817504\n",
            "Epoch 53/100 Loss: 1.3707877397537231 Test Loss: 1.3560525499593974 Test Acc: 0.33153092260898953\n",
            "Epoch 54/100 Loss: 1.349777102470398 Test Loss: 1.3556741344320407 Test Acc: 0.33186887461980397\n",
            "Epoch 55/100 Loss: 1.3150925636291504 Test Loss: 1.3556838074259034 Test Acc: 0.33186887461980397\n",
            "Epoch 56/100 Loss: 1.3657375574111938 Test Loss: 1.3551024557489444 Test Acc: 0.33186887461980397\n",
            "Epoch 57/100 Loss: 1.3569284677505493 Test Loss: 1.355002880942463 Test Acc: 0.33186887461980397\n",
            "Epoch 58/100 Loss: 1.3294236660003662 Test Loss: 1.3543670659292466 Test Acc: 0.33186887461980397\n",
            "Epoch 59/100 Loss: 1.3462077379226685 Test Loss: 1.354220615524261 Test Acc: 0.33186887461980397\n",
            "Epoch 60/100 Loss: 1.3316999673843384 Test Loss: 1.354169386873055 Test Acc: 0.33186887461980397\n",
            "Epoch 61/100 Loss: 1.3941699266433716 Test Loss: 1.3533422222166458 Test Acc: 0.33220682663061846\n",
            "Epoch 62/100 Loss: 1.412546992301941 Test Loss: 1.3536622581066171 Test Acc: 0.33186887461980397\n",
            "Epoch 63/100 Loss: 1.3733807802200317 Test Loss: 1.353084114444703 Test Acc: 0.33186887461980397\n",
            "Epoch 64/100 Loss: 1.344980239868164 Test Loss: 1.352801376357599 Test Acc: 0.33220682663061846\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-530c1483729f>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# 开始训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-530c1483729f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_lengths\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;31m# Keep lengths on CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-c6e35d707903>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# 对输入序列进行填充\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msorted_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msequences_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-c6e35d707903>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# 对输入序列进行填充\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msorted_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msequences_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 自学习阶段\n",
        "high_confidence_threshold = 0.9\n",
        "\n",
        "while True:\n",
        "    high_confidence_samples = []\n",
        "    for i, text in enumerate(dataset_agnews_processed_texts):\n",
        "        if sentence_class_pseudolabels[i] == -1:  # 未标签的数据\n",
        "            with torch.no_grad():\n",
        "                text_tensor = torch.tensor([prepare_sequence(text, dataset_agnews_word2vec)]).float().to(device)\n",
        "                prediction = model(text_tensor, [len(text)])\n",
        "                probabilities, predicted = torch.max(torch.exp(prediction), axis=1)\n",
        "                if probabilities.item() > high_confidence_threshold:\n",
        "                    high_confidence_samples.append((text, predicted.item()))\n",
        "\n",
        "    if len(high_confidence_samples) == 0:\n",
        "        break\n",
        "\n",
        "    # 伪标签数据\n",
        "    X_pseudo_labeled, y_pseudo_labeled = zip(*high_confidence_samples)\n",
        "    X_pseudo_labeled = [prepare_sequence(text, dataset_agnews_word2vec) for text in X_pseudo_labeled]\n",
        "    y_pseudo_labeled = list(y_pseudo_labeled)\n",
        "\n",
        "    # 从未标签的数据池中移除伪标签的数据\n",
        "    dataset_agnews_processed_texts = [text for i, text in enumerate(dataset_agnews_processed_texts) if sentence_class_pseudolabels[i] == -1]\n",
        "    sentence_class_pseudolabels = [label for label in sentence_class_pseudolabels if label == -1]\n",
        "\n",
        "    # 将伪标签数据添加到训练集\n",
        "    train_data.extend(list(zip(X_pseudo_labeled, y_pseudo_labeled)))\n",
        "\n",
        "    # 使用增强的数据集重新创建DataLoader\n",
        "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    # 使用伪标签的数据重新训练模型\n",
        "    train(model, train_loader, test_loader, criterion, optimizer, epochs=5)\n"
      ],
      "metadata": {
        "id": "4jyTnhUOqmo0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}